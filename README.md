#VisionKeys â€” HackHarvard 2025

Repository: adityaraj457/VisionKeys
Event: HackHarvard 2025
Authors: Harry Du, Aditya Raj, Jose Feliz Cuevas
Tagline: VisionKeys â€” an eye and head-controlled typing interface designed to give everyone the power to communicate hands-free.

ğŸš€ Overview

VisionKeys is a web-based eye and head gaze typing system that allows users to type on an on-screen keyboard through eye movement and blinks â€” no hands, no external hardware.
It combines MediaPipeâ€™s Face Landmarker with custom gaze fusion algorithms and a real-time dwell selection system, all running directly in the browser.

ğŸ¯ Built for accessibility, affordability, and assistive innovation â€” VisionKeys empowers users with motor disabilities to communicate using just a webcam.

â­ Key Features

ğŸ‘ï¸ Eye & Head Fusion Tracking â€“ Stable and precise pointer control using both eye and head vectors.

ğŸ’¬ Dwell Typing â€“ Select characters by holding gaze (configurable dwell time).

ğŸ«° Blink Selection â€“ Trigger selection with short, intentional blinks.

ğŸ§² Key Magnetization â€“ Cursor â€œsnapsâ€ to nearest key center to reduce errors.

ğŸ”Š Audio Feedback â€“ Optional sound cues for visual confirmation.

ğŸ§  9-Point Calibration System â€“ Personalized accuracy stored locally.

ğŸ’¾ Client-Side Only â€“ Everything runs in-browser; no data leaves your device.

ğŸª Preview Overlay â€“ Small live camera window showing gaze tracking in real time.

âš¡ Lightweight â€“ Powered by WebAssembly and WebGL for smooth performance.

ğŸ¯ Why VisionKeys

People with limited motor control or temporary hand injuries often face barriers when using traditional input devices.
VisionKeys bridges that gap using only a laptop webcam â€” transforming any device into a powerful assistive communication tool.

At HackHarvard 2025, VisionKeys showcases how accessible AI and computer vision can democratize technology for everyone.

ğŸ§° Tech Stack

HTML / CSS / JavaScript (ES Modules)

MediaPipe Tasks Vision â€“ FaceLandmarker (WASM + GPU)

WebAssembly / WebGL for real-time inference

LocalStorage for calibration persistence

Vanilla JS UI â€” no backend required

ğŸ§  How to Run Locally

Clone the repository

git clone https://github.com/adityaraj457/VisionKeys.git
cd VisionKeys


Start a local development server
(needed because the FaceLandmarker WASM model canâ€™t run from file://)

python -m http.server 8000
# or
npx http-server -p 8000


Open your browser

http://localhost:8000


Allow camera access and press c to calibrate.
Then, start typing with your gaze!

ğŸ›ï¸ Controls & Shortcuts
Key / Action	Function
c	-> Start 9-point calibration
x -> 	Reset calibration data
v ->	Toggle preview camera
t	-> Toggle tracking overlay
] / [	-> Adjust vertical offset
ğŸ”Š Button	Enable or disable sound

âš™ï¸ Calibration & Fine-Tuning

Run c to begin calibration. Follow on-screen dots with your eyes.

Adjust vertical offset with ] or [ keys.

Modify key constants in JS for further control:

DWELL_MS â†’ dwell duration (default 1000 ms)

GAZE_Y_LIFT_PX â†’ vertical lift correction

BLINK_MIN_MS / BLINK_MAX_MS â†’ blink sensitivity

Use v to hide preview if it overlaps with the keyboard.

ğŸŒ Deploying to GitHub Pages

Commit and push your code to GitHub.

Go to: Settings â†’ Pages â†’ Source â†’ Main Branch / Root.

GitHub will automatically host at
ğŸ”— https://adityaraj457.github.io/VisionKeys/

ğŸ” Privacy & Security

VisionKeys processes all data locally â€” no cloud or API calls.

Camera feed never leaves the userâ€™s device.

Calibration data is saved only to browser localStorage.

ğŸ§­ Vision & Social Impact

â€œTyping made possible â€” without hands.â€

VisionKeys is designed to bring independence and dignity to users with limited mobility.
Its low-cost, open-source framework allows schools, clinics, and developers to adopt and improve upon the system easily.

Potential applications:

ALS & paralysis communication aids

Rehabilitation tools

Hands-free input for sterile or hazardous environments

ğŸ§± Future Roadmap

ğŸŒ Multilingual keyboard layouts

ğŸ¤– Local word prediction model integration

âš™ï¸ Adaptive dwell time (AI-adjusted per user)

â˜ï¸ Optional cloud sync for profiles

ğŸ§ª Collaboration with accessibility researchers

ğŸ‘¤ Authors
Name	GitHub / Profile
Harry Du	-----------

Aditya Raj	@adityaraj457

Jose Feliz Cuevas	---------

ğŸ§¾ Quick Summary
git clone https://github.com/Harry-Du1/Eye-tracking-typer
cd VisionKeys
python -m http.server 8000
# Open http://localhost:8000
# Press 'c' to calibrate, 'v' to hide preview, and 'ğŸ”Š' for sound.
